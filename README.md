# Искусственный интеллект

ИИ включает направления:
1. Обработка естественного языка
2. Экспертные системы
3. Речь
4. Компьютерное зрение 
5. Робототехника
6. Автоматическое планирование
7. Машинное обучение.

<img src=pics/00.png>

## 1. Машинное обучение

<img src=pics/01.png>

МО – это область ИИ, когда мы тренируем наш алгоритм с помощью набора данных,
делая его все лучше, точнее и более эффективным. При машинном обучении наши алгоритмы
обучаются на основе данных, но без заранее запрограммированных инструкций. 

<img src=pics/02.png>

Глубокое обучение – это подотрасль МО, то есть здесь тоже компьютер
обучается, но обучается немного по-другому, чем в стандартном МО. В ГО используются нейронные сети (НС), которые представляют собой алгоритмы, повторяющие логику нейронов
человеческого мозга.

<img src=pics/03.png>

Данные бывают:
- маркированные labeled data.
- немаркированные unlabeled data

### 1.1. Обучение с учителем supervised learning

<img src=pics/04.png>

Обучение с учителем включает два типа задач: 
1. регрессия 
2. классификация

#### 1.1.1. Классификация

Разбиение на классы, (известные группы)

Группы методов для решения задач по классификации:
1. Ансамблирование
    - Комитет большинства
    - Выбор среднего
    - Выбор средневзвешенного
2. Бустинг
3. Бэггинг
4. Случайный лес
- Дерево решений
- Логистическая регрессия

- Метод опорных векторов
- Метод К-ближайших соседей

##### 1.1.1.1. Ансамблирование

Это совокупность из нескольких разных моделей для решения одной и той же задачи с целью улучшения точности предсказывания

1. Комитет большинства (теорема Кондорсе о жюри присяжных)
2. Выбор среднего (мудрость толпы)
3. Выбор средневзвешенного

###### 1.1.1.1.1. Комитет большинства

Выбираем то значение из всех моделей, которое встречается наиболее чаще

###### 1.1.1.1.2. Выбор среднего

Выбор средне арифметического из всех решений

###### 1.1.1.1.3. Выбор средневзвешенного

Разным группам зрителей даем разные веса:

- Профессиональные кинокритики - 1
- Опытные пользователи - 0,75
- Обычные пользователи - 0,5

Далее, мы выводим взвешенную среднюю из их оценок

###### 1.1.1.2. Бэггинг

Бэггинг – сокращенное от bootstrap aggregation

Применяют когда изначальная выборка не такая большая, и в связи с этим создают много случайных выборок из исходной выборки.

<img src=pics/07.png>

###### 1.1.1.3. Бустинг

Ансамбль, основанный на последовательной технике, когда каждый последующий алгоритм стремится компенсировать недостатки предыдущих алгоритмов.

Сначала первый алгоритм обучается на всем наборе данных, а каждый последующий алгоритм строится на примерах, в которых предыдущий алгоритм допустил ошибку, таким образом, наблюдениям, которые были предсказаны некорректно предыдущей моделью, дается больший вес.

Пример бустингового метода: AdaBoost

###### 1.1.1.4. Случайный лес

Данный метод применяется не только в классификации, но и регрессии и кластеризации.

<img src=pics/08.png>

В бэггинге мы делали подвыборки и для каждой подвыборки строили дерево решений, основываясь на всех признаках. В случайном лесе, мы также будем делать подвыборки, но кроме этого, в каждой подвыборке мы будем случайным образом брать только отдельные признаки, например, только пол и возраст, или возраст и уровень холестерина.

<img src=pics/09.png>

Далее, как обычно мы будем агрегировать наши результаты и получим еще большую точность предсказаний наших алгоритмов. В задачах классификации нашу финальную модель мы
будем выбирать комитетом большинства, а в задачах регрессии мы будем использовать среднее
арифметическое.

<img src=pics/10.png>

Случайный лес – это как черный ящик, он очень хорошо прогнозирует, но практически ничего не объясняет

#### 1.1.2. Регрессия

Регрессия, когда мы делаем численный прогноз на основе предыдущих данных
Классический пример регрессии – это когда мы предсказываем цену квартиры в зависимости от ее площади

### 1.2. Обучение без учителя unsupervised learning

<img src=pics/05.png>

выводят закономерности и выводы на основе немаркированных данных
такие данные можно обработать двумя способами:
1. Кластеризация
2. Снижение размерности

#### 1.2.1. Кластеризация

<img src=pics/06.png>

в кластеризации мы ничего не предсказываем, а просто распределяем имеющиеся объекты в разные кластеры или группы в зависимости от сходных признаков

#### 1.2.2. Снижение размерности

И снижение размерности, которое необходимо для более удобной демонстрации больших объемов данных.

